{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "012F7lMtkWmG"
      },
      "source": [
        "<h1> KAGGLE ASSIGNMENT - UNSUPERVISED LEARNING </H1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FE0p_FGuTMh"
      },
      "source": [
        "ACF(k) = Σ (x_t - μ)(x_{t-k} - μ) / Σ (x_t - μ)^2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "vDZXIJnChEHM"
      },
      "outputs": [],
      "source": [
        "#21 min to run the whole program\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "import numpy as np\n",
        "import statistics as st\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from scipy.stats import kurtosis, skew\n",
        "\n",
        "#auto correlation function at lag k\n",
        "\n",
        "#it extracts the relationship between past and future data in a time series : about 10 autocorrelations are usually valuable\n",
        "def calculate_auto_corr(time_series_signal,k):\n",
        "    series_mean=np.mean(time_series_signal)\n",
        "\n",
        "    l=len(time_series_signal)\n",
        "\n",
        "    autocorrelation_num=np.sum((time_series_signal[:l-k]-series_mean)*(time_series_signal[k:]-series_mean))\n",
        "    autocorrelation_den=np.sum((time_series_signal-series_mean)**2)\n",
        "\n",
        "    return autocorrelation_num/autocorrelation_den"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2yfVCJgkkNu"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<h3>Reducing No. of Columns, forming 5 sec segments and Projecting Data into higher dimension </h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "5MCjmb44WC50"
      },
      "outputs": [],
      "source": [
        "# 6 min to train\n",
        "train_path=\"/content/Train.csv\"\n",
        "test_path=\"/content/Test.csv\"\n",
        "\n",
        "df_train=pd.read_csv(train_path)\n",
        "\n",
        "df_train_subset=df_train.iloc[:, :-2]\n",
        "data=np.array(df_train_subset).tolist()\n",
        "\n",
        "pre_processed_data=[]\n",
        "for a in data:\n",
        "\n",
        "    new_row=[]\n",
        "\n",
        "    for b in range(0,len(a),125): #taking data 125 points at a time (basically making chunks/segments of 5sec each) then projecting into higher dimension\n",
        "       #6250 cols converted to 50 points\n",
        "\n",
        "       arr=np.array(a[b:b+125])\n",
        "       auto_list=[]    # List to store the autocorrelation features\n",
        "       lags=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
        "\n",
        "\n",
        "       for lag in lags:            # Extract autocorrelation coefficients at the specified lags\n",
        "            curr_lag=calculate_auto_corr(arr,lag)\n",
        "            auto_list.append(curr_lag)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "       mean=np.mean(arr)\n",
        "       mean_ad=np.mean(np.abs(arr-np.mean(arr)))\n",
        "       std=np.std(arr)\n",
        "       iqr=np.percentile(arr,75)-np.percentile(arr,25) #inter-quartile percentile\n",
        "       mad=np.median(np.abs(arr-np.median(arr)))\n",
        "\n",
        "       curr_segment_feature_vector=[mean,np.median(arr),iqr,std,np.var(arr),np.ptp(arr),mad,mean_ad,auto_list[0],auto_list[1],auto_list[2],auto_list[3],auto_list[4],auto_list[5],auto_list[6],auto_list[7],auto_list[8],auto_list[9],auto_list[10],auto_list[11],auto_list[12],auto_list[13],auto_list[14]]\n",
        "       new_row.append(curr_segment_feature_vector)\n",
        "\n",
        "\n",
        "    pre_processed_data.append(new_row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUOchijjolfr"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<h3>Segregating Data based on sensors </h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "JvV8UPJgWHFp"
      },
      "outputs": [],
      "source": [
        "sensors=df_train.iloc[:,-2]\n",
        "\n",
        "\n",
        "#Create a mapping of sensor names to all data-items (values) of that sensor\n",
        "sensor_to_data={}\n",
        "for a in sensors:\n",
        "    if a not in sensor_to_data:\n",
        "       sensor_to_data[a]=[]\n",
        "\n",
        "\n",
        "for i in range(0,len(sensors)):\n",
        "    sensor_to_data[sensors[i]].append(pre_processed_data[i])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iGZiP85pKIr"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "<h3>Train 45 unsupervised KNN Models</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "wEMUqIN7WL7v"
      },
      "outputs": [],
      "source": [
        "# value of k for KNN\n",
        "\n",
        "k=5\n",
        "clusters={}\n",
        "\n",
        "\n",
        "# Fit the model to the data and get cluster assignments for each sensor data. Training 45 models for KNN\n",
        "for a in sensor_to_data:\n",
        "    #152 classes in total and each class has 50 points\n",
        "\n",
        "    pseudo_labels_map=[]\n",
        "    curr_data=sensor_to_data[a]\n",
        "    data_for_knn=[]\n",
        "    cnt=0\n",
        "    for b in curr_data:\n",
        "       data_for_knn.extend(b)\n",
        "\n",
        "       curr_label=[cnt]*50\n",
        "       pseudo_labels_map.extend(curr_label)\n",
        "       cnt+=1\n",
        "\n",
        "\n",
        "\n",
        "    data_for_knn=np.array(data_for_knn)\n",
        "    pseudo_labels_map=np.array(pseudo_labels_map)\n",
        "\n",
        "\n",
        "    #using the unsupervised version of KNN\n",
        "    neigh=NearestNeighbors(n_neighbors=k)\n",
        "    neigh.fit(data_for_knn)\n",
        "    clusters[a]=neigh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbBvrIQnpVka"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<h3>Make a prediction for the testing values after column reduction , segment formation and higher dimension projection</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "_3vk1w05WN8E"
      },
      "outputs": [],
      "source": [
        "# 6min to train\n",
        "\n",
        "df2=pd.read_csv(test_path)\n",
        "df2_subset=df2.iloc[:, :-2]\n",
        "output_data=np.array(df2_subset).tolist()\n",
        "\n",
        "\n",
        "#use majority voting to find the class to be alloted based on k nearest neighbors for the test point\n",
        "def find_class_using_majority_voting(index_list,distances,pseudo_labels_map):\n",
        "\n",
        "    freq_dic={}\n",
        "    for a in index_list[0].tolist():\n",
        "        if a in freq_dic:\n",
        "            freq_dic[a]+=1\n",
        "        else:\n",
        "            freq_dic[a]=1\n",
        "\n",
        "    mode=max(freq_dic,key=freq_dic.get)\n",
        "    ans=mode\n",
        "    return pseudo_labels_map[ans]\n",
        "\n",
        "\n",
        "\n",
        "#generate output list by using the trained models\n",
        "def generate_output_list(row_means2,clusters,sensors2,IDs,k=5):\n",
        "    output_list=[]\n",
        "    for i in range(0,len(row_means2)):\n",
        "        curr_id=IDs[i]\n",
        "        curr_knn=clusters[sensors2[i]]\n",
        "        distances,indices=curr_knn.kneighbors(np.array(row_means2[i]).reshape(1,23), k)\n",
        "        ans=find_class_using_majority_voting(indices,distances,pseudo_labels_map)\n",
        "        output_list.append([curr_id,ans])\n",
        "    return output_list\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "row_means2=[] #test data points (converting 125 cols to one col and projecting to a higher dimension)\n",
        "for a in output_data:\n",
        "\n",
        "\n",
        "    auto_list=[]    # List to store the autocorrelation features\n",
        "    lags= range(1, 16)   # Specify the lags of interest (e.g., lags 1 to 10)\n",
        "\n",
        "    for lag in lags:     # Extract autocorrelation coefficients at the specified lags\n",
        "        curr_lag=calculate_auto_corr(a,lag)\n",
        "        auto_list.append(curr_lag)\n",
        "\n",
        "\n",
        "    mean=np.mean(a)\n",
        "    std=np.std(a)\n",
        "    mad=np.median(np.abs(a-np.median(a)))\n",
        "    mean_ad=np.mean(np.abs(a-np.mean(a)))\n",
        "    iqr=np.percentile(np.array(a),75)-np.percentile(np.array(a),23)\n",
        "\n",
        "    curr_test_feature_vector=[mean,np.median(a),iqr,std,np.var(a),np.ptp(a),mad,mean_ad,auto_list[0],auto_list[1],auto_list[2],auto_list[3],auto_list[4],auto_list[5],auto_list[6],auto_list[7],auto_list[8],auto_list[9],auto_list[10],auto_list[11],auto_list[12],auto_list[13],auto_list[14]]\n",
        "    row_means2.append(curr_test_feature_vector)\n",
        "\n",
        "\n",
        "\n",
        "sensors2=df2.iloc[:,-2]\n",
        "IDs=df2.iloc[:,-1]\n",
        "output_list=generate_output_list(row_means2,clusters,sensors2,IDs)\n",
        "\n",
        "output_df=pd.DataFrame(output_list, columns=['ID', 'TARGET'])\n",
        "\n",
        "\n",
        "csv_file_name ='/content/submission.csv'\n",
        "\n",
        "\n",
        "# Write the DataFrame to a new CSV file\n",
        "output_df.to_csv(csv_file_name, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-TWHcj06vkQ"
      },
      "source": [
        "<br><br>\n",
        "<h3> Starting Google Colab and  downloading datasets </h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73nuOaCJoZuY",
        "outputId": "de64a37c-0d5a-4294-e5c5-bd16e243e67e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading unsupervised-learning-m2023.zip to /content\n",
            " 84% 122M/144M [00:00<00:00, 250MB/s] \n",
            "100% 144M/144M [00:00<00:00, 242MB/s]\n",
            "Archive:  unsupervised-learning-m2023.zip\n",
            "  inflating: Test.csv                \n",
            "  inflating: Train.csv               \n"
          ]
        }
      ],
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json\n",
        "# !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "! kaggle competitions download -c unsupervised-learning-m2023\n",
        "! unzip unsupervised-learning-m2023.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UF6CHMiCY-x"
      },
      "outputs": [],
      "source": [
        "#1D\n",
        "# Direct 1D distance formula on median (6250 cols-> 1col ) -> 12%\n",
        "# KMeans -> <10%\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# test data is also of the form mean, median (125 cols->1 col)\n",
        "\n",
        "#2D\n",
        "# KNN - point, point (6250 cols->6250 cols with k=3125) -> 13%\n",
        "# KNN - 50 set means, medians (6250 cols ->125 cols with k-63) -> 23%\n",
        "# KNN - 125 set means, medians (6250 cols->50 cols with k=25) -> 26%    increasing k more than n/2 doesn't increase accuracy more\n",
        "# KNN - 250 set means, medians (6250 cols->25 cols with k=13) -> 25%\n",
        "\n",
        "#3/4/and higher dimensional\n",
        "# KNN - 125 set means, medians , mode (6250 cols->50 cols with k=25) -> 21%\n",
        "# KNN - 125 set means, medians , mode , iqr (6250 cols->50 cols with k=25) -> 29%\n",
        "# KNN - 125 set means, medians , iqr (6250 cols->50 cols with k=25) -> 35%\n",
        "# KNN - 125 set means, medians , iqr , standard dev (6250 cols->50 cols with k=25) ->\n",
        "# KNN - 125 set means, medians , iqr , var  (6250 cols->50 cols with k=25) ->\n",
        "# KNN - 125 set means, medians , iqr , standard dev , var (6250 cols->50 cols with k=25) -> 37%\n",
        "# -------------------------------------------------------------------------------------------------------\n",
        "# KNN - 125 set means, medians , iqr , standard dev , var , range (6250 cols->50 cols with k=63)  -> 38%       (sub 23)\n",
        "# KNN - 125 set means, medians , iqr , standard dev , var , range, COV (6250 cols->50 cols with k=63)  ->           (sub 24)\n",
        "# KNN - 125 set means, medians , iqr , standard dev , var , COV (6250 cols->50 cols with k=63)  -> 22%                 (sub 22)\n",
        "# KNN - 125 set means, medians , iqr , standard dev , var , COV,range (6250 cols->50 cols with k=63)  -> --           (sub 21)\n",
        "# KNN - 125 set means, medians , iqr , var (6250 cols->50 cols with k=63)  -> 37%           (sub 21)\n",
        "# -----------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# KNN - 125 set means, medians , iqr , standard dev , var , range (6250 cols->50 cols with k=25)  -> 40%        bestie\n",
        "# ------------------------------------------------------------------------------------------------------------\n",
        "# KNN - 125 set means, medians , iqr , standard dev , var , range (6250 cols->50 cols with k=45)  -> 38%\n",
        "\n",
        "\n",
        "\n",
        "#try increasing test data cols also   X not a good idea\n",
        "# KNN - 125 set means, medians , var , range , mad (6250 cols->50 cols with k=25)  ->  23%    [5 test cols]\n",
        "# KNN - 125 set means, medians , std, var , range , mad (6250 cols->50 cols with k=25)  ->  28%    [5 test cols with corner = case]\n",
        "# ------------------------------------------------------------------------------\n",
        "# KNN - 125 set means, medians , std, var , range  (6250 cols->50 cols with k=25)  ->   16%   [125 test cols with corner = case]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Non linear SVMs\n",
        "\n",
        "# Non linear SVM - 125 set means, medians , iqr , standard dev , var , range (6250 cols->50 cols with gamma='scale' and C=1)  -> 28% (SUB 35)\n",
        "# Non linear SVM - 50 set means, medians , iqr , standard dev , var , range (6250 cols->125 cols with gamma='scale' and C=1)  ->\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Random Forests\n",
        "\n",
        "# Random Forest - 125 set means, medians , iqr , standard dev , var , range (6250 cols->50 cols with n_estimators=100 , random_state=42)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
